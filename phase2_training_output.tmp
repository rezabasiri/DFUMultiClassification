2025-12-27 17:45:27.633179: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-12-27 17:45:27.633332: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-12-27 17:45:27.635008: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-12-27 17:45:28.395758: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT

================================================================================
DEVICE CONFIGURATION (mode: single)
================================================================================

Detected 2 GPU(s):
  GPU 0: NVIDIA GeForce RTX 5090 - 31.8GB
  GPU 1: NVIDIA GeForce RTX 5090 - 31.8GB

Selected GPU 0: NVIDIA GeForce RTX 5090 (31.8GB)
2025-12-27 17:45:32.543233: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2348] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
Enabled memory growth for 1 GPU(s)

Using default strategy (single GPU)
================================================================================


Batch size per replica: 16 (global batch size: 16, replicas: 1)

================================================================================
DFU MULTIMODAL CLASSIFICATION - PRODUCTION PIPELINE
================================================================================
Mode: search
Resume mode: fresh
Data percentage: 100.0%
Verbosity: 0 (MINIMAL)
Device: GPU 0 (single GPU mode)
Cross-validation: 3-fold CV (patient-level)

Configuration loaded from: src/utils/production_config.py
Image size: 128x128
Batch size: 16
Max epochs: 150 (with early stopping)
Modality search mode: custom
Will test 1 custom combinations
================================================================================


ðŸ§¹ FRESH START MODE: Deleting all checkpoints...
================================================================================

Cleanup Statistics:
  Csv Results: 5 files deleted
  Tf Cache: 4 files deleted
================================================================================

2025-12-27 17:45:33.361501: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2348] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
Misclassification filtering thresholds: {'I': 16, 'P': 18, 'R': 20}

============================================================
FILTERING SUMMARY
============================================================
Thresholds: I=16, P=18, R=20

Excluded samples per class:
  Class I: 62 samples
  Class P: 66 samples
  Class R: 6 samples

Total unique samples to exclude: 134

Dataset size (rows): 3107 -> 2327 (74.9%)
Unique samples: 647 -> 513 (removed 134)

Class distribution after filtering:
  Class I: 542 rows
  Class P: 1483 rows
  Class R: 302 rows
============================================================

2025-12-27 17:45:38.739016: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2348] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2025-12-27 17:45:40.483312: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2025-12-27 17:48:38.528990: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1766857728.920188 2063227 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2025-12-27 17:49:23.157095: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2025-12-27 17:50:32.092408: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2025-12-27 17:52:06.643180: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Error during training (attempt 1/3): Graph execution error:

Detected at node IteratorGetNext defined at (most recent call last):
  File "/workspace/DFUMultiClassification/src/main.py", line 2486, in <module>

  File "/workspace/DFUMultiClassification/src/main.py", line 2107, in main

  File "/workspace/DFUMultiClassification/src/main.py", line 1860, in main_search

  File "/workspace/DFUMultiClassification/src/training/training_utils.py", line 1214, in cross_validation_manual_split

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 65, in error_handler

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 1856, in fit

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 65, in error_handler

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 2296, in evaluate

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 4108, in run_step

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 2066, in test_function

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 2048, in step_function

Detected at node IteratorGetNext defined at (most recent call last):
  File "/workspace/DFUMultiClassification/src/main.py", line 2486, in <module>

  File "/workspace/DFUMultiClassification/src/main.py", line 2107, in main

  File "/workspace/DFUMultiClassification/src/main.py", line 1860, in main_search

  File "/workspace/DFUMultiClassification/src/training/training_utils.py", line 1214, in cross_validation_manual_split

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 65, in error_handler

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 1856, in fit

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 65, in error_handler

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 2296, in evaluate

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 4108, in run_step

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 2066, in test_function

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 2048, in step_function

2 root error(s) found.
  (0) NOT_FOUND:  /workspace/DFUMultiClassification/results/tf_records/tf_cache_valid_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate8096648016978113055; No such file or directory
	 [[{{node IteratorGetNext}}]]
	 [[IteratorGetNext/_4]]
  (1) NOT_FOUND:  /workspace/DFUMultiClassification/results/tf_records/tf_cache_valid_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate8096648016978113055; No such file or directory
	 [[{{node IteratorGetNext}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_test_function_127405]
Traceback: Traceback (most recent call last):
  File "/workspace/DFUMultiClassification/src/training/training_utils.py", line 1214, in cross_validation_manual_split
    history = model.fit(
              ^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    except TypeError as e:
tensorflow.python.framework.errors_impl.NotFoundError: Graph execution error:

Detected at node IteratorGetNext defined at (most recent call last):
  File "/workspace/DFUMultiClassification/src/main.py", line 2486, in <module>

  File "/workspace/DFUMultiClassification/src/main.py", line 2107, in main

  File "/workspace/DFUMultiClassification/src/main.py", line 1860, in main_search

  File "/workspace/DFUMultiClassification/src/training/training_utils.py", line 1214, in cross_validation_manual_split

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 65, in error_handler

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 1856, in fit

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 65, in error_handler

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 2296, in evaluate

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 4108, in run_step

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 2066, in test_function

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 2048, in step_function

Detected at node IteratorGetNext defined at (most recent call last):
  File "/workspace/DFUMultiClassification/src/main.py", line 2486, in <module>

  File "/workspace/DFUMultiClassification/src/main.py", line 2107, in main

  File "/workspace/DFUMultiClassification/src/main.py", line 1860, in main_search

  File "/workspace/DFUMultiClassification/src/training/training_utils.py", line 1214, in cross_validation_manual_split

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 65, in error_handler

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 1856, in fit

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 65, in error_handler

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 2296, in evaluate

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 4108, in run_step

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 2066, in test_function

  File "/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/training.py", line 2048, in step_function

2 root error(s) found.
  (0) NOT_FOUND:  /workspace/DFUMultiClassification/results/tf_records/tf_cache_valid_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate8096648016978113055; No such file or directory
	 [[{{node IteratorGetNext}}]]
	 [[IteratorGetNext/_4]]
  (1) NOT_FOUND:  /workspace/DFUMultiClassification/results/tf_records/tf_cache_valid_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate8096648016978113055; No such file or directory
	 [[{{node IteratorGetNext}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_test_function_127405]

2025-12-27 17:53:13.861073: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2025-12-27 17:53:13.876650: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Error during training (attempt 2/3): {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate15788711588149113558; No such file or directory [Op:IteratorGetNext] name: 
Traceback: Traceback (most recent call last):
  File "/workspace/DFUMultiClassification/src/training/training_utils.py", line 1043, in cross_validation_manual_split
    for batch in pre_aug_train_dataset.take(master_steps_per_epoch):
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 810, in __next__
    return self._next_internal()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 773, in _next_internal
    ret = gen_dataset_ops.iterator_get_next(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 3029, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/framework/ops.py", line 5883, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.NotFoundError: {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate15788711588149113558; No such file or directory [Op:IteratorGetNext] name: 

2025-12-27 17:56:24.993753: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Error during training (attempt 3/3): {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate6057905027246761359; No such file or directory [Op:IteratorGetNext] name: 
Traceback: Traceback (most recent call last):
  File "/workspace/DFUMultiClassification/src/training/training_utils.py", line 1043, in cross_validation_manual_split
    for batch in pre_aug_train_dataset.take(master_steps_per_epoch):
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 810, in __next__
    return self._next_internal()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 773, in _next_internal
    ret = gen_dataset_ops.iterator_get_next(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 3029, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/framework/ops.py", line 5883, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.NotFoundError: {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate6057905027246761359; No such file or directory [Op:IteratorGetNext] name: 

2025-12-27 17:59:21.969377: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
ERROR: Training failed for metadata+depth_rgb+depth_map+thermal_map after 3 attempts. Skipping this configuration.
Warning: No predictions to save for run 1 (train). Skipping...
Warning: No predictions to save for run 1 (valid). Skipping...
Error during training (attempt 1/3): {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate16492003738539787379; No such file or directory [Op:IteratorGetNext] name: 
Traceback: Traceback (most recent call last):
  File "/workspace/DFUMultiClassification/src/training/training_utils.py", line 1043, in cross_validation_manual_split
    for batch in pre_aug_train_dataset.take(master_steps_per_epoch):
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 810, in __next__
    return self._next_internal()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 773, in _next_internal
    ret = gen_dataset_ops.iterator_get_next(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 3029, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/framework/ops.py", line 5883, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.NotFoundError: {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate16492003738539787379; No such file or directory [Op:IteratorGetNext] name: 

2025-12-27 18:02:57.250447: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Error during training (attempt 2/3): {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate10650134925554460115; No such file or directory [Op:IteratorGetNext] name: 
Traceback: Traceback (most recent call last):
  File "/workspace/DFUMultiClassification/src/training/training_utils.py", line 1043, in cross_validation_manual_split
    for batch in pre_aug_train_dataset.take(master_steps_per_epoch):
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 810, in __next__
    return self._next_internal()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 773, in _next_internal
    ret = gen_dataset_ops.iterator_get_next(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 3029, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/framework/ops.py", line 5883, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.NotFoundError: {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate10650134925554460115; No such file or directory [Op:IteratorGetNext] name: 

2025-12-27 18:06:28.861374: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Error during training (attempt 3/3): {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate1482412793425908887; No such file or directory [Op:IteratorGetNext] name: 
Traceback: Traceback (most recent call last):
  File "/workspace/DFUMultiClassification/src/training/training_utils.py", line 1043, in cross_validation_manual_split
    for batch in pre_aug_train_dataset.take(master_steps_per_epoch):
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 810, in __next__
    return self._next_internal()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 773, in _next_internal
    ret = gen_dataset_ops.iterator_get_next(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 3029, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/framework/ops.py", line 5883, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.NotFoundError: {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate1482412793425908887; No such file or directory [Op:IteratorGetNext] name: 

2025-12-27 18:10:03.753475: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
ERROR: Training failed for metadata+depth_rgb+depth_map+thermal_map after 3 attempts. Skipping this configuration.
Warning: No predictions to save for run 2 (train). Skipping...
Warning: No predictions to save for run 2 (valid). Skipping...
Error during training (attempt 1/3): {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate16123496254513728817; No such file or directory [Op:IteratorGetNext] name: 
Traceback: Traceback (most recent call last):
  File "/workspace/DFUMultiClassification/src/training/training_utils.py", line 1043, in cross_validation_manual_split
    for batch in pre_aug_train_dataset.take(master_steps_per_epoch):
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 810, in __next__
    return self._next_internal()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 773, in _next_internal
    ret = gen_dataset_ops.iterator_get_next(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 3029, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/framework/ops.py", line 5883, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.NotFoundError: {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate16123496254513728817; No such file or directory [Op:IteratorGetNext] name: 

2025-12-27 18:13:47.253170: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Error during training (attempt 2/3): {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate1938551043344970060; No such file or directory [Op:IteratorGetNext] name: 
Traceback: Traceback (most recent call last):
  File "/workspace/DFUMultiClassification/src/training/training_utils.py", line 1043, in cross_validation_manual_split
    for batch in pre_aug_train_dataset.take(master_steps_per_epoch):
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 810, in __next__
    return self._next_internal()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 773, in _next_internal
    ret = gen_dataset_ops.iterator_get_next(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 3029, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/framework/ops.py", line 5883, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.NotFoundError: {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate1938551043344970060; No such file or directory [Op:IteratorGetNext] name: 

2025-12-27 18:17:16.376499: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Error during training (attempt 3/3): {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate16810591378026337625; No such file or directory [Op:IteratorGetNext] name: 
Traceback: Traceback (most recent call last):
  File "/workspace/DFUMultiClassification/src/training/training_utils.py", line 1043, in cross_validation_manual_split
    for batch in pre_aug_train_dataset.take(master_steps_per_epoch):
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 810, in __next__
    return self._next_internal()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 773, in _next_internal
    ret = gen_dataset_ops.iterator_get_next(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 3029, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File "/venv/multimodal/lib/python3.11/site-packages/tensorflow/python/framework/ops.py", line 5883, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.NotFoundError: {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_depth_map_depth_rgb_metadata_thermal_map_0.data-00000-of-00001.tempstate16810591378026337625; No such file or directory [Op:IteratorGetNext] name: 

2025-12-27 18:20:44.036369: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
ERROR: Training failed for metadata+depth_rgb+depth_map+thermal_map after 3 attempts. Skipping this configuration.
Warning: No predictions to save for run 3 (train). Skipping...
Warning: No predictions to save for run 3 (valid). Skipping...
/venv/multimodal/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/venv/multimodal/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/venv/multimodal/lib/python3.11/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/venv/multimodal/lib/python3.11/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide
  arrmean = um.true_divide(arrmean, div, out=arrmean,
/venv/multimodal/lib/python3.11/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)

All results saved to /workspace/DFUMultiClassification/results/csv/modality_combination_results.csv

================================================================================
FINAL SUMMARY - BEST MODALITY COMBINATIONS
================================================================================
/workspace/DFUMultiClassification/src/main.py:2040: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError
  best_acc_idx = results_df['Accuracy (Mean)'].idxmax()
================================================================================

2025-12-27 18:20:45.763169: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2348] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
