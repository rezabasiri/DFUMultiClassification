2026-01-20 21:33:06.959781: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-01-20 21:33:06.959845: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-01-20 21:33:06.961173: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-01-20 21:33:06.965574: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-01-20 21:33:06.965637: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-01-20 21:33:06.966952: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-01-20 21:33:06.967363: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-20 21:33:06.972960: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-20 21:33:07.688442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2026-01-20 21:33:07.688680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
================================================================================
Training Configuration: configs/full_sdxl.yaml
Phase: all
Modality: rgb
Resolution: 512
================================================================================
Loading base model: stabilityai/stable-diffusion-xl-base-1.0
Detected SDXL model - loading dual text encoders
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Enabled gradient checkpointing
/venv/multimodal/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/venv/multimodal/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
LoRA Configuration:
  Rank: 64
  Alpha: 128
  Trainable parameters: 167,444,480 (6.12%)
/venv/multimodal/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/venv/multimodal/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Enabled perceptual loss
Found 2860 images for rgb/all phases
  Phase I: 860 images
  Phase P: 1678 images
  Phase R: 322 images
Using phase-specific prompts for phases: ['I', 'P', 'R']
Stratified split: 2430 train, 430 validation
  Phase I: 731 train, 129 val (85.0%/15.0%)
  Phase P: 1426 train, 252 val (85.0%/15.0%)
  Phase R: 273 train, 49 val (84.8%/15.2%)
Enabled EMA (on CPU to save GPU memory)
Using phase-specific prompts for conditioning
Found 2860 images for rgb/all phases
  Phase I: 860 images
  Phase P: 1678 images
  Phase R: 322 images
Using phase-specific prompts for phases: ['I', 'P', 'R']
Stratified split: 2430 train, 430 validation
  Phase I: 731 train, 129 val (85.0%/15.0%)
  Phase P: 1426 train, 252 val (85.0%/15.0%)
  Phase R: 273 train, 49 val (84.8%/15.2%)
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
/venv/multimodal/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
/venv/multimodal/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Loading model from: /venv/multimodal/lib/python3.11/site-packages/lpips/weights/v0.1/alex.pth
/venv/multimodal/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.
  warnings.warn(*args, **kwargs)
Loading model from: /venv/multimodal/lib/python3.11/site-packages/lpips/weights/v0.1/alex.pth

Starting training...
Training on 2 GPUs
Total epochs: 200
Effective batch size: 16

Epoch 1/200
  Step 1/303 - Loss: 0.8742, LR: 0.00e+00
  Step 100/303 - Loss: 1.1928, LR: 4.00e-05
  Step 200/303 - Loss: 1.1601, LR: 8.00e-05
  Step 300/303 - Loss: 1.1445, LR: 1.00e-04
  Epoch 1 complete - Loss: 1.1430, LR: 1.00e-04
Train loss: 1.1430
  Validation: 54 batches - Loss: 0.1214
Val loss: 0.1214
Loaded 16 reference images for phase I
Loaded 16 reference images for phase P
Loaded 16 reference images for phase R
Computing quality metrics...
Generating 50 images across 3 phases...
  Phase I: generating 17 images...
  Phase P: generating 17 images...
  Phase R: generating 16 images...
Computing FID...
/venv/multimodal/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
Computing SSIM...
Computing LPIPS...
Computing Inception Score...

Per-phase metrics:
==================================================
Phase I: SSIM=0.3813, LPIPS=0.7975
Phase P: SSIM=0.3277, LPIPS=0.7921
Phase R: SSIM=0.4017, LPIPS=0.7970
==================================================
Quality Metrics:
==================================================
FID: 363.30 (lower is better, < 50 is good)
SSIM: 0.3875 ± 0.1188 (higher is better, > 0.7 is good)
LPIPS: 0.7849 ± 0.0337 (lower is better, < 0.3 is good)
IS: 3.07 ± 0.48 (higher is better, > 2.0 is good)
==================================================
Val FID: 363.30
[rank0]: Traceback (most recent call last):
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/serialization.py", line 967, in save
[rank0]:     _save(
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/serialization.py", line 1268, in _save
[rank0]:     zip_file.write_record(name, storage, num_bytes)
[rank0]: RuntimeError: [enforce fail at inline_container.cc:858] . PytorchStreamWriter failed writing file data/560: file write failed

[rank0]: During handling of the above exception, another exception occurred:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/DFUMultiClassification/agent_communication/generative_augmentation/scripts/train_lora_model.py", line 1278, in <module>
[rank0]:     main()
[rank0]:   File "/workspace/DFUMultiClassification/agent_communication/generative_augmentation/scripts/train_lora_model.py", line 1246, in main
[rank0]:     checkpoint_manager.save_checkpoint(
[rank0]:   File "/workspace/DFUMultiClassification/agent_communication/generative_augmentation/scripts/utils/checkpoint_utils.py", line 133, in save_checkpoint
[rank0]:     torch.save(checkpoint, checkpoint_path)
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/serialization.py", line 966, in save
[rank0]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/serialization.py", line 798, in __exit__
[rank0]:     self.file_like.write_end_of_file()
[rank0]: RuntimeError: [enforce fail at inline_container.cc:664] . unexpected pos 1532462656 vs 1532462544
[rank0]:[W120 21:55:07.385485314 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W120 21:55:08.367391551 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
W0120 21:55:10.403000 2462752 site-packages/torch/distributed/elastic/multiprocessing/api.py:908] Sending process 2462972 closing signal SIGTERM
E0120 21:55:12.572000 2462752 site-packages/torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 2462971) of binary: /venv/multimodal/bin/python3.11
Traceback (most recent call last):
  File "/venv/multimodal/bin/accelerate", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/venv/multimodal/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1272, in launch_command
    multi_gpu_launcher(args)
  File "/venv/multimodal/lib/python3.11/site-packages/accelerate/commands/launch.py", line 899, in multi_gpu_launcher
    distrib_run.run(args)
  File "/venv/multimodal/lib/python3.11/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/venv/multimodal/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts/train_lora_model.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-20_21:55:10
  host      : 8c8dadf7e27a
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2462971)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
