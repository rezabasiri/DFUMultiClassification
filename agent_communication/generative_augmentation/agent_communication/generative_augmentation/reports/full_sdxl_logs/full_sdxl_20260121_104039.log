Logging to: agent_communication/generative_augmentation/reports/full_sdxl_logs/full_sdxl_20260121_104039.log
================================================================================
Training Configuration: configs/full_sdxl.yaml
Phase: all
Modality: rgb
Resolution: 256
================================================================================
Loading base model: stabilityai/stable-diffusion-xl-base-1.0
Detected SDXL model - loading dual text encoders
Enabled gradient checkpointing
LoRA Configuration:
  Rank: 128
  Alpha: 256
  Trainable parameters: 334,888,960 (11.54%)
/venv/multimodal/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/venv/multimodal/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Enabled perceptual loss
Enabled EMA (on CPU to save GPU memory)
Using phase-specific prompts for conditioning
Loaded 2884 bounding boxes from /workspace/DFUMultiClassification/data/raw/bounding_box_depth.csv
Found 2860 images for rgb/all phases
  Phase I: 860 images
  Phase P: 1678 images
  Phase R: 322 images
Using phase-specific prompts for phases: ['I', 'P', 'R']
Stratified split: 2573 train, 287 validation
  Phase I: 774 train, 86 val (90.0%/10.0%)
  Phase P: 1510 train, 168 val (90.0%/10.0%)
  Phase R: 289 train, 33 val (89.8%/10.2%)
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
/venv/multimodal/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Loading model from: /venv/multimodal/lib/python3.11/site-packages/lpips/weights/v0.1/alex.pth
/venv/multimodal/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.
  warnings.warn(*args, **kwargs)

Starting training...
Training on 5 GPUs
Total epochs: 300
Effective batch size: 80

Epoch 1/300
  Step 1/257 - Loss: 0.5903, LR: 0.00e+00
  Step 100/257 - Loss: 3.2368, LR: 6.00e-07
  Step 200/257 - Loss: 3.1209, LR: 1.25e-06
  Epoch 1 complete - Loss: 3.0995, LR: 1.65e-06
Train loss: 3.0995
  Validation: 29 batches - Loss: 0.1788
Val loss: 0.1788
Loaded 16 reference images for phase I
Loaded 16 reference images for phase P
Loaded 16 reference images for phase R
Loaded reference images from disk: agent_communication/generative_augmentation/reports/full_sdxl_logs/reference_images
Computing quality metrics...
Generating 50 images across 3 phases...
  Phase I: generating 17 images...
  Phase P: generating 17 images...
  Phase R: generating 16 images...
Computing FID...
/venv/multimodal/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
Computing SSIM...
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/DFUMultiClassification/agent_communication/generative_augmentation/scripts/train_lora_model.py", line 1398, in <module>
[rank0]:     main()
[rank0]:   File "/workspace/DFUMultiClassification/agent_communication/generative_augmentation/scripts/train_lora_model.py", line 1233, in main
[rank0]:     val_metrics = quality_metrics.compute_all_metrics(
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/DFUMultiClassification/agent_communication/generative_augmentation/scripts/utils/quality_metrics.py", line 352, in compute_all_metrics
[rank0]:     mean_ssim, ssim_scores = self.compute_ssim_batch(generated_images, real_images)
[rank0]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/DFUMultiClassification/agent_communication/generative_augmentation/scripts/utils/quality_metrics.py", line 208, in compute_ssim_batch
[rank0]:     ssim = self.compute_ssim(gen_img, real_img)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/DFUMultiClassification/agent_communication/generative_augmentation/scripts/utils/quality_metrics.py", line 181, in compute_ssim
[rank0]:     ssim_score = self.ssim_metric(image1, image2).item()
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torchmetrics/metric.py", line 315, in forward
[rank0]:     self._forward_cache = self._forward_reduce_state_update(*args, **kwargs)
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torchmetrics/metric.py", line 384, in _forward_reduce_state_update
[rank0]:     self.update(*args, **kwargs)
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torchmetrics/metric.py", line 559, in wrapped_func
[rank0]:     raise err
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torchmetrics/metric.py", line 549, in wrapped_func
[rank0]:     update(*args, **kwargs)
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torchmetrics/image/ssim.py", line 131, in update
[rank0]:     preds, target = _ssim_check_inputs(preds, target)
[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torchmetrics/functional/image/ssim.py", line 37, in _ssim_check_inputs
[rank0]:     _check_same_shape(preds, target)
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torchmetrics/utilities/checks.py", line 39, in _check_same_shape
[rank0]:     raise RuntimeError(
[rank0]: RuntimeError: Predictions and targets are expected to have the same shape, but got torch.Size([1, 3, 256, 256]) and torch.Size([1, 3, 32, 32]).
