2026-01-19 13:31:39.588373: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-01-19 13:31:39.588434: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-01-19 13:31:39.589597: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-01-19 13:31:39.595166: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-19 13:31:39.637711: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-01-19 13:31:39.637771: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-01-19 13:31:39.638990: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-01-19 13:31:39.644509: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-19 13:31:40.286401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2026-01-19 13:31:40.359056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
================================================================================
Training Configuration: configs/test_sdxl.yaml
Phase: I
Modality: rgb
Resolution: 32
================================================================================
Loading base model: stabilityai/stable-diffusion-xl-base-1.0
Detected SDXL model - loading dual text encoders
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Enabled gradient checkpointing
LoRA Configuration:
  Rank: 16
  Alpha: 32
  Trainable parameters: 23,224,320 (0.90%)
/venv/multimodal/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/venv/multimodal/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/venv/multimodal/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/venv/multimodal/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Enabled perceptual loss
Enabled EMA
Found 50 images for rgb/I
Dataset split: 42 train, 8 validation
Limited train set to 16 samples for fast testing
Found 50 images for rgb/I
Dataset split: 42 train, 8 validation
Limited train set to 16 samples for fast testing
Loaded 10 reference images from /workspace/DFUMultiClassification/data/DFU_Updated/rgb/I
Loaded 10 reference images from /workspace/DFUMultiClassification/data/DFU_Updated/rgb/I
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
/venv/multimodal/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
/venv/multimodal/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Loading model from: /venv/multimodal/lib/python3.11/site-packages/lpips/weights/v0.1/alex.pth
Loading model from: /venv/multimodal/lib/python3.11/site-packages/lpips/weights/v0.1/alex.pth
/venv/multimodal/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.
  warnings.warn(*args, **kwargs)

Starting training...
Training on 2 GPUs
Total epochs: 2
Effective batch size: 4

Epoch 1/2
  Epoch 1 complete - Loss: 17.9887, LR: 1.00e-05
Train loss: 17.9887
  Validation: 2 batches - Loss: 0.3490
[SYNC DEBUG] Process 0 moving quality metrics to GPU...
[SYNC DEBUG] Process 0 offloading training models to CPU...
[SYNC DEBUG] Process 1 moving quality metrics to GPU...
[SYNC DEBUG] Process 1 offloading training models to CPU...
[SYNC DEBUG] Process 1 finished offloading models
[SYNC DEBUG] Non-main process waiting for generated images...
[SYNC DEBUG] Process 1 broadcasting generated images...
[SYNC DEBUG] Process 1 received generated images: torch.Size([5, 3, 32, 32])
[SYNC DEBUG] Process 1 computing metrics...
Computing FID...
[FID DEBUG] Starting FID computation
[FID DEBUG] Real images shape: torch.Size([10, 3, 32, 32])
[FID DEBUG] Generated images shape: torch.Size([5, 3, 32, 32])
[FID DEBUG] Moved images to device: cuda:1
[FID DEBUG] After RGB conversion - Real: torch.Size([10, 3, 32, 32]), Gen: torch.Size([5, 3, 32, 32])
[FID DEBUG] Converted to uint8
[FID DEBUG] Updating FID metric with real images...
[SYNC DEBUG] Process 0 finished offloading models
Computing quality metrics...
  Generating samples for FID computation...
[SYNC DEBUG] Process 0 broadcasting generated images...
[SYNC DEBUG] Process 0 received generated images: torch.Size([5, 3, 32, 32])
[SYNC DEBUG] Process 0 computing metrics...
Computing FID...
[FID DEBUG] Starting FID computation
[FID DEBUG] Real images shape: torch.Size([10, 3, 32, 32])
[FID DEBUG] Generated images shape: torch.Size([5, 3, 32, 32])
[FID DEBUG] Moved images to device: cuda:0
[FID DEBUG] After RGB conversion - Real: torch.Size([10, 3, 32, 32]), Gen: torch.Size([5, 3, 32, 32])
[FID DEBUG] Converted to uint8
[FID DEBUG] Updating FID metric with real images...
[FID DEBUG] Real images updated successfully
[FID DEBUG] Updating FID metric with generated images...
[FID DEBUG] Real images updated successfully
[FID DEBUG] Updating FID metric with generated images...
[FID DEBUG] Generated images updated successfully
[FID DEBUG] Calling fid_metric.compute()...
/venv/multimodal/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[FID DEBUG] Generated images updated successfully
[FID DEBUG] Calling fid_metric.compute()...
[FID DEBUG] FID computed successfully: 424.671630859375
[FID DEBUG] Resetting FID metric...
[FID DEBUG] FID metric reset successfully
Computing SSIM...
Computing LPIPS...
[FID DEBUG] FID computed successfully: 424.671630859375
[FID DEBUG] Resetting FID metric...
[FID DEBUG] FID metric reset successfully
Computing SSIM...
Computing Inception Score...
Computing LPIPS...
Computing Inception Score...
[SYNC DEBUG] Process 0 finished computing metrics
[SYNC DEBUG] Process 0 cleaning up memory...
[SYNC DEBUG] Process 1 finished computing metrics
[SYNC DEBUG] Process 1 cleaning up memory...
[SYNC DEBUG] Process 1 calling wait_for_everyone()...
[SYNC DEBUG] Process 0 calling wait_for_everyone()...
[SYNC DEBUG] Process 1 passed wait_for_everyone()
[SYNC DEBUG] Process 1 restoring training models to GPU...
[SYNC DEBUG] Process 0 passed wait_for_everyone()
Quality Metrics:
==================================================
FID: 424.67 (lower is better, < 50 is good)
SSIM: 0.0820 ± 0.0496 (higher is better, > 0.7 is good)
LPIPS: 0.3252 ± 0.0717 (lower is better, < 0.3 is good)
IS: 1.00 ± 0.00 (higher is better, > 2.0 is good)
==================================================
Val loss: 0.3490, Val FID: 424.67
[SYNC DEBUG] Process 0 restoring training models to GPU...
[SYNC DEBUG] Process 1 restoring UNet...
[SYNC DEBUG] Process 0 restoring UNet...
[SYNC DEBUG] Process 0 restoring VAE...
[SYNC DEBUG] Process 1 restoring VAE...
[SYNC DEBUG] Process 0 restoring text_encoder...
[SYNC DEBUG] Process 1 restoring text_encoder...
[SYNC DEBUG] Process 0 restoring text_encoder_2...
[SYNC DEBUG] Process 1 restoring text_encoder_2...
[SYNC DEBUG] Process 0 restoring EMA model...
[SYNC DEBUG] Process 1 restoring EMA model...
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/DFUMultiClassification/agent_communication/generative_augmentation/scripts/train_lora_model.py", line 1068, in <module>
[rank0]:     main()
[rank0]:   File "/workspace/DFUMultiClassification/agent_communication/generative_augmentation/scripts/train_lora_model.py", line 999, in main
[rank0]:     ema_model.ema_model.to(original_device)
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1371, in to
[rank0]:     return self._apply(convert)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
[rank0]:     module._apply(fn)
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
[rank0]:     module._apply(fn)
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
[rank0]:     module._apply(fn)
[rank0]:   [Previous line repeated 8 more times]
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/nn/modules/module.py", line 957, in _apply
[rank0]:     param_applied = fn(param)
[rank0]:                     ^^^^^^^^^
[rank0]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1357, in convert
[rank0]:     return t.to(
[rank0]:            ^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 23.52 GiB of which 28.69 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.46 GiB is allocated by PyTorch, and 421.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/workspace/DFUMultiClassification/agent_communication/generative_augmentation/scripts/train_lora_model.py", line 1068, in <module>
[rank1]:     main()
[rank1]:   File "/workspace/DFUMultiClassification/agent_communication/generative_augmentation/scripts/train_lora_model.py", line 999, in main
[rank1]:     ema_model.ema_model.to(original_device)
[rank1]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1371, in to
[rank1]:     return self._apply(convert)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
[rank1]:     module._apply(fn)
[rank1]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
[rank1]:     module._apply(fn)
[rank1]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/nn/modules/module.py", line 930, in _apply
[rank1]:     module._apply(fn)
[rank1]:   [Previous line repeated 8 more times]
[rank1]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/nn/modules/module.py", line 957, in _apply
[rank1]:     param_applied = fn(param)
[rank1]:                     ^^^^^^^^^
[rank1]:   File "/venv/multimodal/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1357, in convert
[rank1]:     return t.to(
[rank1]:            ^^^^^
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 1 has a total capacity of 23.52 GiB of which 30.69 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.46 GiB is allocated by PyTorch, and 421.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W119 13:32:53.564776193 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0119 13:32:56.133000 2210234 site-packages/torch/distributed/elastic/multiprocessing/api.py:908] Sending process 2210443 closing signal SIGTERM
E0119 13:32:56.298000 2210234 site-packages/torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 2210442) of binary: /venv/multimodal/bin/python3.11
Traceback (most recent call last):
  File "/venv/multimodal/bin/accelerate", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/venv/multimodal/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1272, in launch_command
    multi_gpu_launcher(args)
  File "/venv/multimodal/lib/python3.11/site-packages/accelerate/commands/launch.py", line 899, in multi_gpu_launcher
    distrib_run.run(args)
  File "/venv/multimodal/lib/python3.11/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/venv/multimodal/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/venv/multimodal/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts/train_lora_model.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-19_13:32:56
  host      : 8c8dadf7e27a
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2210442)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
