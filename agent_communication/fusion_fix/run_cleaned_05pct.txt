2026-01-05 18:30:13.825109: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-01-05 18:30:13.825157: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-01-05 18:30:13.826685: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-01-05 18:30:14.787280: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/venv/multimodal/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/venv/multimodal/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/venv/multimodal/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(

================================================================================
DEVICE CONFIGURATION (mode: single)
================================================================================

Detected 8 GPU(s):
  GPU 0: NVIDIA GeForce RTX 4090 - 24.0GB (compute 8.9)
  GPU 1: NVIDIA GeForce RTX 4090 - 24.0GB (compute 8.9)
  GPU 2: NVIDIA GeForce RTX 4090 - 24.0GB (compute 8.9)
  GPU 3: NVIDIA GeForce RTX 4090 - 24.0GB (compute 8.9)
  GPU 4: NVIDIA GeForce RTX 4090 - 24.0GB (compute 8.9)
  GPU 5: NVIDIA GeForce RTX 4090 - 24.0GB (compute 8.9)
  GPU 6: NVIDIA GeForce RTX 4090 - 24.0GB (compute 8.9)
  GPU 7: NVIDIA GeForce RTX 4090 - 24.0GB (compute 8.9)

Selected GPU 0: NVIDIA GeForce RTX 4090 (24.0GB)
Enabled memory growth for 1 GPU(s)

Using default strategy (single GPU)
================================================================================


Batch size per replica: 320 (global batch size: 320, replicas: 1)

================================================================================
DFU MULTIMODAL CLASSIFICATION - PRODUCTION PIPELINE
================================================================================
Mode: search
Resume mode: auto
Data percentage: 100.0%
Verbosity: 1 (NORMAL)
Device: GPU 0 (single GPU mode)
Cross-validation: 3-fold CV (patient-level)

Configuration loaded from: src/utils/production_config.py
Image size: 32x32
Batch size: 320
Max epochs: 300 (with early stopping)
Modality search mode: custom
Will test 1 custom combinations
================================================================================


✨ AUTO RESUME MODE: Keeping all checkpoints, will resume from latest state...
================================================================================

================================================================================
MODALITY SEARCH MODE: CUSTOM (1 combinations)
================================================================================
Testing only specified combinations from production_config.py
Total combinations to test: 1
Cross-validation mode: 3-fold CV
Iterations per combination: 3
Total training sessions: 3
Results will be saved to: /workspace/DFUMultiClassification/results/csv/modality_combination_results.csv
================================================================================


Testing modalities: metadata, thermal_map
Number of samples for each selected modality:
  thermal_map: 2929
  thermal_bb: 2929
  metadata: 2929
Using 100.0% of the data: 2929 samples

================================================================================
GENERATING 3-FOLD CROSS-VALIDATION SPLITS (PATIENT-LEVEL)
================================================================================
Fold 1/3: 151 train patients, 76 valid patients
  Train dist: {0: 0.302, 1: 0.589, 2: 0.109}
  Valid dist: {0: 0.245, 1: 0.648, 2: 0.107}
Fold 2/3: 151 train patients, 76 valid patients
  Train dist: {0: 0.262, 1: 0.615, 2: 0.122}
  Valid dist: {0: 0.321, 1: 0.598, 2: 0.081}
Fold 3/3: 152 train patients, 75 valid patients
  Train dist: {0: 0.283, 1: 0.623, 2: 0.094}
  Valid dist: {0: 0.281, 1: 0.580, 2: 0.140}
Generated 3 folds
All data will be validated exactly once across all folds
================================================================================


Fold 1/3
Using pre-computed fold 1 patient split
Using Scikit-learn RandomForestClassifier
2026-01-05 18:30:35.379850: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.

Preparing datasets for Fold 1/3 with all modalities: ['thermal_map', 'metadata']
Using consistent patient split across all modality combinations for run 1
Using Scikit-learn RandomForestClassifier

No existing data found for metadata+thermal_map, starting fresh

Training metadata+thermal_map with modalities: ['metadata', 'thermal_map'], fold 1/3
2026-01-05 18:33:45.872714: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
================================================================================
AUTOMATIC PRE-TRAINING: thermal_map weights not found
  Training thermal_map-only model first (same data split)...
================================================================================
/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['metadata_input'] which did not match any model input. They will be ignored by the model.
  inputs = self._flatten_to_reference_inputs(inputs)
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1767638033.092442 3043053 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2026-01-05 18:33:56.002097: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2026-01-05 18:34:52.493665: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Restoring model weights from the end of the best epoch: 64.
Epoch 84: early stopping
  Pre-training completed! Best val kappa: 0.1045
  WARNING: 0 trainable parameters! This will prevent learning!
================================================================================
No existing pretrained weights found
Restoring model weights from the end of the best epoch: 1.
Epoch 11: early stopping
Restoring model weights from the end of the best epoch: 1.
Epoch 11: early stopping

Run 1 Results for metadata+thermal_map:
              precision    recall  f1-score   support

           I       0.55      0.39      0.46       246
           P       0.72      0.82      0.76       650
           R       0.31      0.25      0.28       107

    accuracy                           0.65      1003
   macro avg       0.53      0.49      0.50      1003
weighted avg       0.63      0.65      0.64      1003

Cohen's Kappa: 0.3170

Training gating network for run 1...

Initializing gating network training...
Skipping gating network: only 1 model(s), need at least 2 to combine

Gating Network Results for Run 1:
Accuracy: 0.6520
F1 Macro: 0.5005
Kappa: 0.3170

Fold 2/3
Using pre-computed fold 2 patient split
Using Scikit-learn RandomForestClassifier
2026-01-05 18:36:13.939700: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.

Preparing datasets for Fold 2/3 with all modalities: ['thermal_map', 'metadata']
Using consistent patient split across all modality combinations for run 2
Using Scikit-learn RandomForestClassifier

No existing data found for metadata+thermal_map, starting fresh

Training metadata+thermal_map with modalities: ['metadata', 'thermal_map'], fold 2/3
2026-01-05 18:39:41.625865: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
================================================================================
AUTOMATIC PRE-TRAINING: thermal_map weights not found
  Training thermal_map-only model first (same data split)...
================================================================================
/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['metadata_input'] which did not match any model input. They will be ignored by the model.
  inputs = self._flatten_to_reference_inputs(inputs)
2026-01-05 18:39:47.931601: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2026-01-05 18:40:35.574846: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Restoring model weights from the end of the best epoch: 14.
Epoch 34: early stopping
  Pre-training completed! Best val kappa: 0.0845
  WARNING: 0 trainable parameters! This will prevent learning!
================================================================================
No existing pretrained weights found
Restoring model weights from the end of the best epoch: 1.
Epoch 11: early stopping
Restoring model weights from the end of the best epoch: 1.
Epoch 11: early stopping

Run 2 Results for metadata+thermal_map:
              precision    recall  f1-score   support

           I       0.34      0.29      0.31       324
           P       0.60      0.67      0.63       604
           R       0.14      0.11      0.12        82

    accuracy                           0.50      1010
   macro avg       0.36      0.36      0.36      1010
weighted avg       0.48      0.50      0.49      1010

Cohen's Kappa: 0.0563

Training gating network for run 2...

Initializing gating network training...
Skipping gating network: only 1 model(s), need at least 2 to combine

Gating Network Results for Run 2:
Accuracy: 0.5020
F1 Macro: 0.3562
Kappa: 0.0563

Fold 3/3
Using pre-computed fold 3 patient split
Using Scikit-learn RandomForestClassifier
2026-01-05 18:41:32.437023: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.

Preparing datasets for Fold 3/3 with all modalities: ['thermal_map', 'metadata']
Using consistent patient split across all modality combinations for run 3
Using Scikit-learn RandomForestClassifier

No existing data found for metadata+thermal_map, starting fresh

Training metadata+thermal_map with modalities: ['metadata', 'thermal_map'], fold 3/3
2026-01-05 18:45:14.491862: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
================================================================================
AUTOMATIC PRE-TRAINING: thermal_map weights not found
  Training thermal_map-only model first (same data split)...
================================================================================
/venv/multimodal/lib/python3.11/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['metadata_input'] which did not match any model input. They will be ignored by the model.
  inputs = self._flatten_to_reference_inputs(inputs)
2026-01-05 18:45:20.493287: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2026-01-05 18:46:12.169097: W tensorflow/core/kernels/data/cache_dataset_ops.cc:302] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Restoring model weights from the end of the best epoch: 4.
Epoch 24: early stopping
  Pre-training completed! Best val kappa: 0.1438
  WARNING: 0 trainable parameters! This will prevent learning!
================================================================================
No existing pretrained weights found
Restoring model weights from the end of the best epoch: 1.
Epoch 11: early stopping
Restoring model weights from the end of the best epoch: 1.
Epoch 11: early stopping

Run 3 Results for metadata+thermal_map:
              precision    recall  f1-score   support

           I       0.35      0.49      0.41       257
           P       0.61      0.45      0.52       531
           R       0.38      0.50      0.43       128

    accuracy                           0.47       916
   macro avg       0.45      0.48      0.46       916
weighted avg       0.51      0.47      0.48       916

Cohen's Kappa: 0.2817

Training gating network for run 3...

Initializing gating network training...
Skipping gating network: only 1 model(s), need at least 2 to combine

Gating Network Results for Run 3:
Accuracy: 0.4705
F1 Macro: 0.4555
Kappa: 0.2817
Results saved to /workspace/DFUMultiClassification/results/csv/modality_results_averaged.csv
Saved metadata+thermal_map predictions to /workspace/DFUMultiClassification/results/checkpoints/combo_pred_metadata_thermal_map_run1_train.npy
Saved metadata+thermal_map predictions to /workspace/DFUMultiClassification/results/checkpoints/combo_pred_metadata_thermal_map_run1_valid.npy
Saved metadata+thermal_map predictions to /workspace/DFUMultiClassification/results/checkpoints/combo_pred_metadata_thermal_map_run2_train.npy
Saved metadata+thermal_map predictions to /workspace/DFUMultiClassification/results/checkpoints/combo_pred_metadata_thermal_map_run2_valid.npy
Saved metadata+thermal_map predictions to /workspace/DFUMultiClassification/results/checkpoints/combo_pred_metadata_thermal_map_run3_train.npy
Saved metadata+thermal_map predictions to /workspace/DFUMultiClassification/results/checkpoints/combo_pred_metadata_thermal_map_run3_valid.npy
Results for metadata, thermal_map appended to /workspace/DFUMultiClassification/results/csv/modality_combination_results.csv

All results saved to /workspace/DFUMultiClassification/results/csv/modality_combination_results.csv

================================================================================
FINAL SUMMARY - BEST MODALITY COMBINATIONS
================================================================================

Best by Accuracy:
  Modalities: metadata+thermal_map
  Accuracy: 0.5415 ± 0.0792
  F1 Macro: 0.4374
  Kappa: 0.2183

Best by F1 Macro:
  Modalities: metadata+thermal_map
  Accuracy: 0.5149
  F1 Macro: 0.4389 ± 0.0201
  Kappa: 0.2786

Total combinations tested: 3
================================================================================

