[LOG] Output is being logged to: /workspace/DFUMultiClassification/results/logs/confidence_fold1.log
[LOG] Started at: 2026-02-13 21:30:59


================================================================================
DEVICE CONFIGURATION (mode: multi)
================================================================================

Detected 2 GPU(s):
  GPU 0: NVIDIA RTX A5000 - 24.0GB (compute 8.6)
  GPU 1: NVIDIA RTX A5000 - 24.0GB (compute 8.6)

Selected 2 GPU(s):
  GPU 0: NVIDIA RTX A5000 (24.0GB)
  GPU 1: NVIDIA RTX A5000 (24.0GB)
Enabled memory growth for 2 GPU(s)

Using MirroredStrategy (2 GPUs) with NCCL
  Compute capability 8.6 detected (native NCCL support)
  (Using NCCL for fastest multi-GPU communication)
  Effective batch size: 2Ã— global batch size
================================================================================


Batch size per replica: 16 (global batch size: 32, replicas: 2)

================================================================================
DFU MULTIMODAL CLASSIFICATION - PRODUCTION PIPELINE
================================================================================
Mode: search
Resume mode: fresh
Data percentage: 100.0%
Verbosity: 2 (DETAILED)
Device: GPUs [0, 1] (multi-GPU mode, MirroredStrategy)
  Replicas: 2Ã— batch size distribution
Cross-validation: 3-fold CV (patient-level)

Configuration loaded from: src/utils/production_config.py
Image size: 256x256
Batch size: 32
  Per-GPU batch: 16 (32 / 2 GPUs)
Max epochs: 200 (with early stopping)
Modality search mode: custom
Will test 1 custom combinations
================================================================================


ðŸ§¹ FRESH START MODE: Deleting all checkpoints...
================================================================================

Cleanup Statistics:
  Predictions: 12 files deleted
  Csv Results: 7 files deleted
  Tf Cache: 4 files deleted
================================================================================


Data Cleaning Configuration (from production_config.py):
  Outlier removal: False
  Misclassification tracking: none
Outlier removal disabled
Confidence filtering disabled (DISABLE_CONFIDENCE_FILTERING=1)

================================================================================
MODALITY SEARCH MODE: CUSTOM (1 combinations)
================================================================================
Testing only specified combinations from production_config.py
Total combinations to test: 1
Cross-validation mode: 3-fold CV
Iterations per combination: 3
Total training sessions: 3
Results will be saved to: /workspace/DFUMultiClassification/results/csv/modality_combination_results.csv
================================================================================


Testing modalities: metadata, depth_rgb
DEBUG CONF-FILTER: CONFIDENCE_EXCLUSION_FILE env = None
DEBUG CONF-FILTER: File exists = N/A
Number of samples for each selected modality:
  depth_rgb: 3108
  depth_bb: 3108
  metadata: 3108
Using 100.0% of the data: 3108 samples
Using strategy from main: MirroredStrategy

================================================================================
MULTI-GPU TRAINING: 2 GPUs
Global batch size: 32 (per-GPU: 16)
Strategy: MirroredStrategy
================================================================================


================================================================================
GENERATING 3-FOLD CROSS-VALIDATION SPLITS (PATIENT-LEVEL)
================================================================================

================================================================================
STRATIFIED K-FOLD SPLIT - CLASS DISTRIBUTION CHECK
================================================================================
Class 0: 87 patients
Class 1: 115 patients
Class 2: 31 patients
================================================================================

Fold 1/3: 154 train patients, 79 valid patients
  Train dist: {0: 0.312, 1: 0.606, 2: 0.082}
  Valid dist: {0: 0.248, 1: 0.603, 2: 0.149}
Fold 2/3: 156 train patients, 77 valid patients
  Train dist: {0: 0.258, 1: 0.613, 2: 0.129}
  Valid dist: {0: 0.350, 1: 0.587, 2: 0.063}
Fold 3/3: 156 train patients, 77 valid patients
  Train dist: {0: 0.294, 1: 0.596, 2: 0.110}
  Valid dist: {0: 0.272, 1: 0.626, 2: 0.103}
Generated 3 folds
All data will be validated exactly once across all folds
================================================================================


Fold 1/3
Using pre-computed fold 1 patient split
Processing metadata shape...

Unique cases: 518 (before oversampling)

True binary label distributions (unique cases):
Binary1: label_bin1
1    352
0    166
Name: count, dtype: int64
Binary2: label_bin2
0    457
1     61
Name: count, dtype: int64
Original class distribution (ordered):
Class 0: 728
Class 1: 1485
Class 2: 267

Calculated alpha values from original distribution:
Alpha values (ordered) [I, P, R]: [0.711, 0.349, 1.94]
Original class distribution (ordered):
Class 0: 728
Class 1: 1485
Class 2: 267
Using combined sampling (under + over)...

After undersampling (ordered):
Class 0: 728
Class 1: 728
Class 2: 267

After oversampling (ordered):
Class 0: 728
Class 1: 728
Class 2: 728

Alpha values after oversampling [I, P, R]: [1.0, 1.0, 1.0]
(Should be close to [1, 1, 1] since data is balanced)
Feature selection: 54 â†’ 40 features
Top 5 features: ['Peri-Ulcer Temperature Normalized (Â°C)', 'Wound Centre Temperature Normalized (Â°C)', 'Onset (Days)', 'BMI', 'Weight (Kg)']
Using Scikit-learn RandomForestClassifier
Using 40 features from training selection
Healing Phase Abs type: <class 'numpy.ndarray'> shape: (2184,)
Healing Phase Abs type: <class 'numpy.ndarray'> shape: (628,)
  Validation batch size adjusted: 1 â†’ 2 (n_samples=628, num_gpus=2)
Setting image shapes to 256x256...
Generative augmentation disabled

Preparing datasets for Fold 1/3 with all modalities: ['depth_rgb', 'metadata']
Using consistent patient split across all modality combinations for run 1

Class distributions:
Training: {0: 0.312, 1: 0.606, 2: 0.082}
Validation: {0: 0.248, 1: 0.603, 2: 0.149}

Unique cases: 407 (before oversampling)

True binary label distributions (unique cases):
Binary1: label_bin1
1    270
0    137
Name: count, dtype: int64
Binary2: label_bin2
0    371
1     36
Name: count, dtype: int64
Original class distribution (ordered):
Class 0: 599
Class 1: 1164
Class 2: 158

Calculated alpha values from original distribution:
Alpha values (ordered) [I, P, R]: [0.565, 0.291, 2.144]
Original class distribution (ordered):
Class 0: 599
Class 1: 1164
Class 2: 158
Using combined sampling (under + over)...

After undersampling (ordered):
Class 0: 599
Class 1: 599
Class 2: 158

After oversampling (ordered):
Class 0: 599
Class 1: 599
Class 2: 599

Alpha values after oversampling [I, P, R]: [1.0, 1.0, 1.0]
(Should be close to [1, 1, 1] since data is balanced)
Feature selection: 54 â†’ 40 features
Top 5 features: ['Wound Centre Temperature Normalized (Â°C)', 'Peri-Ulcer Temperature Normalized (Â°C)', 'Onset (Days)', 'BMI', 'Weight (Kg)']
Using Scikit-learn RandomForestClassifier
Using 40 features from training selection
depth_rgb type: <class 'pandas.arrays.StringArray'> shape: (1797,)
Healing Phase Abs type: <class 'numpy.ndarray'> shape: (1797,)
Healing Phase Abs type: <class 'numpy.ndarray'> shape: (1187,)
depth_rgb type: <class 'pandas.arrays.StringArray'> shape: (1187,)
  Validation batch size adjusted: 32 â†’ 32 (n_samples=1187, num_gpus=2)
  [diagnostic] Saved class I sample: /workspace/DFUMultiClassification/results/visualizations/diagnostic_samples/fold1_classI_P053_A03_D1.png
  [diagnostic] Saved class P sample: /workspace/DFUMultiClassification/results/visualizations/diagnostic_samples/fold1_classP_P058_A00_D1.png
  [diagnostic] Saved class R sample: /workspace/DFUMultiClassification/results/visualizations/diagnostic_samples/fold1_classR_P270_A00_D1.png
  Diagnostic samples saved to /workspace/DFUMultiClassification/results/visualizations/diagnostic_samples/ (3 classes)

No existing data found for metadata+depth_rgb, starting fresh

Training metadata+depth_rgb with modalities: ['metadata', 'depth_rgb'], fold 1/3
Alpha values (ordered) [I, P, R]: [1.0, 1.0, 1.0]
Class weights: {0: 1, 1: 1, 2: 1} or [1, 1, 1]

Creating image branch for depth_rgb
depth_rgb using backbone: EfficientNetB3
Loading EfficientNetB3 from ImageNet
depth_rgb using EfficientNetB3: 338 trainable weights
Model: Metadata + 1 image - two-stage fine-tuning with pre-trained image
  Fusion weights: LEARNABLE (initialized RF=0.70, Image=0.30)
  Fusion model with 1 image modalities: ['depth_rgb']
  âœ— Missing pre-trained weights for: ['depth_rgb']
    These modalities will be trained from scratch
================================================================================
AUTOMATIC PRE-TRAINING: 1 modality(ies) need training
  Missing modalities: ['depth_rgb']
  Will train each modality separately (same data split)...
================================================================================
================================================================================
PRE-TRAINING 1/1: depth_rgb
================================================================================

Creating image branch for depth_rgb
depth_rgb using backbone: EfficientNetB3
depth_rgb using EfficientNetB3: 338 trainable weights
  Pre-training depth_rgb-only on same data split (prevents data leakage)
Epoch 1/200 - 36.3s - loss: 1.5757 - val_loss: 1.1683 - acc: 0.4046 - val_acc: 0.3522 - macro_f1: 0.3851 - val_macro_f1: 0.3081 - kappa: 0.1048 - val_kappa: 0.0620
Epoch 20/200 - 6.0s - loss: 0.4972 - val_loss: 0.5648 - acc: 0.5658 - val_acc: 0.4443 - macro_f1: 0.5527 - val_macro_f1: 0.4046 - kappa: 0.3497 - val_kappa: 0.1163
Epoch 40/200 - 6.1s - loss: 0.4508 - val_loss: 0.6026 - acc: 0.6075 - val_acc: 0.4071 - macro_f1: 0.6035 - val_macro_f1: 0.3794 - kappa: 0.4108 - val_kappa: 0.0781
Epoch 45: early stopping
Restoring model weights from the end of the best epoch: 25.
  depth_rgb pre-training completed! Best val kappa: 0.1554
  Checkpoint saved to: /workspace/DFUMultiClassification/results/models/depth_rgb_1_depth_rgb.weights.h5
  Transferring depth_rgb pre-trained weights to fusion model...
  Successfully transferred 13 layers for depth_rgb!
  Saved depth_rgb pre-trained weights to cache
================================================================================
FREEZING ALL IMAGE BRANCHES FOR STAGE 1
  Pre-trained modalities to freeze: ['depth_rgb']
================================================================================
  Successfully frozen 13 layers across 1 modalities!
  Two-stage training: Stage 1 (frozen, 20 epochs) â†’ Stage 2 (fine-tune, LR=1e-05)
  DEBUG: Trainable weights breakdown after freezing:
    image_classifier: 2 trainable weights
    output: 1 trainable weights
  Total trainable parameters across all layers: 3
  DEBUG: Checking RF metadata predictions...
    Sample RF predictions (first 5): [[0.27340516 0.6336803  0.09291457]
 [0.89549804 0.10155082 0.00295113]
 [0.35678276 0.64321727 0.        ]
 [0.9074999  0.07897058 0.01352953]
 [0.29687354 0.6427917  0.06033479]]
    RF predictions sum to 1.0: [1.0, 1.0, 1.0]
    Sample labels (first 5): [[0. 1. 0.]
 [1. 0. 0.]
 [0. 1. 0.]
 [1. 0. 0.]
 [0. 1. 0.]]
================================================================================
No existing pretrained weights found
Total model trainable weights: 3
================================================================================
STAGE 1: Training with FROZEN image branch (20 epochs)
  Goal: Stabilize fusion layer before fine-tuning image
================================================================================
Epoch 15: early stopping
Restoring model weights from the end of the best epoch: 5.
  Stage 1 completed. Best val kappa: 0.0651
================================================================================
STAGE 2: Fine-tuning with UNFROZEN image branches (1 modalities)
  Image modalities to unfreeze: ['depth_rgb']
  Learning rate: 1e-6 (very low to prevent overfitting)
  Unfreezing image layers...
================================================================================
  Successfully unfrozen 14 layers across 1 modalities
  Model recompiled with LR=1e-05
Epoch 11: early stopping
Restoring model weights from the end of the best epoch: 1.
================================================================================
Two-stage training completed!
  Stage 1 (frozen):    Kappa 0.0651
  Stage 2 (fine-tune): Kappa 0.0636
  No improvement from fine-tuning (kept Stage 1 weights)
================================================================================
