[LOG] Output is being logged to: /workspace/DFUMultiClassification/results/logs/training_fold2.log
[LOG] Started at: 2026-02-13 18:10:09


================================================================================
DEVICE CONFIGURATION (mode: multi)
================================================================================

Detected 2 GPU(s):
  GPU 0: NVIDIA RTX A5000 - 24.0GB (compute 8.6)
  GPU 1: NVIDIA RTX A5000 - 24.0GB (compute 8.6)

Selected 2 GPU(s):
  GPU 0: NVIDIA RTX A5000 (24.0GB)
  GPU 1: NVIDIA RTX A5000 (24.0GB)
Enabled memory growth for 2 GPU(s)

Using MirroredStrategy (2 GPUs) with NCCL
  Compute capability 8.6 detected (native NCCL support)
  (Using NCCL for fastest multi-GPU communication)
  Effective batch size: 2× global batch size
================================================================================


Batch size per replica: 300 (global batch size: 600, replicas: 2)

================================================================================
DFU MULTIMODAL CLASSIFICATION - PRODUCTION PIPELINE
================================================================================
Mode: search
Resume mode: auto
Data percentage: 40.0%
Verbosity: 2 (DETAILED)
Device: GPUs [0, 1] (multi-GPU mode, MirroredStrategy)
  Replicas: 2× batch size distribution
Cross-validation: 2-fold CV (patient-level)

Configuration loaded from: src/utils/production_config.py
Image size: 256x256
Batch size: 600
  Per-GPU batch: 300 (600 / 2 GPUs)
Max epochs: 200 (with early stopping)
Modality search mode: custom
Will test 1 custom combinations
================================================================================


✨ AUTO RESUME MODE: Keeping all checkpoints, will resume from latest state...
================================================================================

Data Cleaning Configuration (from production_config.py):
  Outlier removal: False
  Misclassification tracking: none
Outlier removal disabled
Confidence filtering skipped (running specific fold subprocess)

================================================================================
MODALITY SEARCH MODE: CUSTOM (1 combinations)
================================================================================
Testing only specified combinations from production_config.py
Total combinations to test: 1
Cross-validation mode: 2-fold CV
Iterations per combination: 2
Total training sessions: 2
Results will be saved to: /workspace/DFUMultiClassification/results/csv/modality_combination_results.csv
================================================================================


Testing modalities: metadata, depth_rgb
DEBUG CONF-FILTER: CONFIDENCE_EXCLUSION_FILE env = /workspace/DFUMultiClassification/results/confidence_exclusion_list.txt
DEBUG CONF-FILTER: File exists = True
DEBUG CONF-FILTER: Loaded 109 excluded IDs from file
DEBUG CONF-FILTER: Sample IDs in data (first 3): ['P092A01D1', 'P092A01D1', 'P005A04D1']
DEBUG CONF-FILTER: Sample IDs in exclusion (first 3): ['P077A00D1', 'P090A03D1', 'P067A03D1']
DEBUG CONF-FILTER: Matched & excluded 526 samples
Confidence filtering: excluded 526/3108 samples (16.9%)
Number of samples for each selected modality:
  depth_rgb: 2582
  depth_bb: 2582
  metadata: 2582
Using 40.0% of the data: 1033 samples
Using strategy from main: MirroredStrategy

================================================================================
MULTI-GPU TRAINING: 2 GPUs
Global batch size: 600 (per-GPU: 300)
Strategy: MirroredStrategy
================================================================================


================================================================================
GENERATING 2-FOLD CROSS-VALIDATION SPLITS (PATIENT-LEVEL)
================================================================================

================================================================================
STRATIFIED K-FOLD SPLIT - CLASS DISTRIBUTION CHECK
================================================================================
Class 0: 72 patients
Class 1: 98 patients
Class 2: 32 patients
================================================================================

Fold 1/2: 101 train patients, 101 valid patients
  Train dist: {0: 0.312, 1: 0.599, 2: 0.089}
  Valid dist: {0: 0.299, 1: 0.552, 2: 0.149}
Fold 2/2: 101 train patients, 101 valid patients
  Train dist: {0: 0.299, 1: 0.552, 2: 0.149}
  Valid dist: {0: 0.312, 1: 0.599, 2: 0.089}
Generated 2 folds
All data will be validated exactly once across all folds
================================================================================


Fold 1/2
Using pre-computed fold 1 patient split

Fold 1/2 skipped (target_fold=2). Loading saved results...
  Loaded 1 saved metric(s) for Fold 1/2

Fold 2/2
Using pre-computed fold 2 patient split
Processing metadata shape...

Unique cases: 383 (before oversampling)

True binary label distributions (unique cases):
Binary1: label_bin1
1    268
0    115
Name: count, dtype: int64
Binary2: label_bin2
0    328
1     55
Name: count, dtype: int64
Original class distribution (ordered):
Class 0: 239
Class 1: 478
Class 2: 107

Calculated alpha values from original distribution:
Alpha values (ordered) [I, P, R]: [0.804, 0.402, 1.795]
Original class distribution (ordered):
Class 0: 239
Class 1: 478
Class 2: 107
Using combined sampling (under + over)...

After undersampling (ordered):
Class 0: 239
Class 1: 239
Class 2: 107

After oversampling (ordered):
Class 0: 239
Class 1: 239
Class 2: 239

Using frequency-based weights from ORIGINAL distribution:
Alpha values [I, P, R]: [0.804, 0.402, 1.795]
(These weights emphasize minority classes even after resampling)
Feature selection: 54 → 40 features
Top 5 features: ['BMI', 'Peri-Ulcer Temperature Normalized (°C)', 'Onset (Days)', 'Wound Centre Temperature Normalized (°C)', 'Intact Skin Temperature (°C)']
Using Scikit-learn RandomForestClassifier
Using 40 features from training selection
Healing Phase Abs type: <class 'numpy.ndarray'> shape: (717,)
Healing Phase Abs type: <class 'numpy.ndarray'> shape: (209,)
  Validation batch size adjusted: 1 → 2 (n_samples=209, num_gpus=2)
Setting image shapes to 256x256...
Removed cache file: /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_metadata_fold0_seed42.data-00000-of-00001
Removed cache file: /workspace/DFUMultiClassification/results/tf_records/tf_cache_train_metadata_fold0_seed42.index
Generative augmentation disabled

Preparing datasets for Fold 2/2 with all modalities: ['depth_rgb', 'metadata']
Using consistent patient split across all modality combinations for run 2

Class distributions:
Training: {0: 0.299, 1: 0.552, 2: 0.149}
Validation: {0: 0.312, 1: 0.599, 2: 0.089}

Unique cases: 248 (before oversampling)

True binary label distributions (unique cases):
Binary1: label_bin1
1    170
0     78
Name: count, dtype: int64
Binary2: label_bin2
0    213
1     35
Name: count, dtype: int64
Original class distribution (ordered):
Class 0: 158
Class 1: 292
Class 2: 79

Calculated alpha values from original distribution:
Alpha values (ordered) [I, P, R]: [0.847, 0.458, 1.694]
Original class distribution (ordered):
Class 0: 158
Class 1: 292
Class 2: 79
Using combined sampling (under + over)...

After undersampling (ordered):
Class 0: 158
Class 1: 158
Class 2: 79

After oversampling (ordered):
Class 0: 158
Class 1: 158
Class 2: 158

Using frequency-based weights from ORIGINAL distribution:
Alpha values [I, P, R]: [0.847, 0.458, 1.694]
(These weights emphasize minority classes even after resampling)
Feature selection: 54 → 40 features
Top 5 features: ['Peri-Ulcer Temperature Normalized (°C)', 'Onset (Days)', 'BMI', 'Wound Centre Temperature Normalized (°C)', 'Intact Skin Temperature (°C)']
Using Scikit-learn RandomForestClassifier
Using 40 features from training selection
depth_rgb type: <class 'pandas.arrays.StringArray'> shape: (474,)
Healing Phase Abs type: <class 'numpy.ndarray'> shape: (474,)
Healing Phase Abs type: <class 'numpy.ndarray'> shape: (504,)
depth_rgb type: <class 'pandas.arrays.StringArray'> shape: (504,)
  Validation batch size adjusted: 600 → 504 (n_samples=504, num_gpus=2)

No existing data found for metadata+depth_rgb, starting fresh

Training metadata+depth_rgb with modalities: ['metadata', 'depth_rgb'], fold 2/2
Alpha values (ordered) [I, P, R]: [0.847, 0.458, 1.694]
Class weights: {0: 1, 1: 1, 2: 1} or [1, 1, 1]

Creating image branch for depth_rgb
depth_rgb using backbone: EfficientNetB3
Loading EfficientNetB3 from ImageNet
depth_rgb using EfficientNetB3: 338 trainable weights
Model: Metadata + 1 image - two-stage fine-tuning with pre-trained image
  Fusion weights: RF=0.70, Image=0.30
  Fusion model with 1 image modalities: ['depth_rgb']
  ✗ Missing pre-trained weights for: ['depth_rgb']
    These modalities will be trained from scratch
================================================================================
AUTOMATIC PRE-TRAINING: 1 modality(ies) need training
  Missing modalities: ['depth_rgb']
  Will train each modality separately (same data split)...
================================================================================
================================================================================
PRE-TRAINING 1/1: depth_rgb
================================================================================

Creating image branch for depth_rgb
depth_rgb using backbone: EfficientNetB3
depth_rgb using EfficientNetB3: 338 trainable weights
  Pre-training depth_rgb-only on same data split (prevents data leakage)
Epoch 1/200 - 33.4s - loss: 2.0931 - val_loss: 1.9536 - acc: 0.3417 - val_acc: 0.1230 - macro_f1: 0.3402 - val_macro_f1: 0.1155 - kappa: 0.0117 - val_kappa: 0.0151
Epoch 20/200 - 2.6s - loss: 1.9992 - val_loss: 1.8924 - acc: 0.4267 - val_acc: 0.0893 - macro_f1: 0.4142 - val_macro_f1: 0.0546 - kappa: 0.1398 - val_kappa: 0.0000
Epoch 21: early stopping
Restoring model weights from the end of the best epoch: 1.
  depth_rgb pre-training completed! Best val kappa: 0.0151
  Checkpoint saved to: /workspace/DFUMultiClassification/results/models/depth_rgb_2_depth_rgb.weights.h5
  Transferring depth_rgb pre-trained weights to fusion model...
  Successfully transferred 13 layers for depth_rgb!
  Saved depth_rgb pre-trained weights to cache
================================================================================
FREEZING ALL IMAGE BRANCHES FOR STAGE 1
  Pre-trained modalities to freeze: ['depth_rgb']
================================================================================
  Successfully frozen 13 layers across 1 modalities!
  Two-stage training: Stage 1 (frozen, 20 epochs) → Stage 2 (fine-tune, LR=1e-6)
  DEBUG: Trainable weights breakdown after freezing:
    image_classifier: 2 trainable weights
  Total trainable parameters across all layers: 2
  DEBUG: Checking RF metadata predictions...
    Sample RF predictions (first 5): [[0.0741174  0.04723049 0.8786521 ]
 [0.01665136 0.00505676 0.97829187]
 [0.8183825  0.1145413  0.06707618]
 [0.139945   0.06676567 0.7932893 ]
 [0.40462252 0.5301696  0.06520784]]
    RF predictions sum to 1.0: [1.0, 1.0, 1.0]
    Sample labels (first 5): [[0. 0. 1.]
 [0. 0. 1.]
 [1. 0. 0.]
 [0. 0. 1.]
 [0. 1. 0.]]
================================================================================
No existing pretrained weights found
Total model trainable weights: 2
================================================================================
STAGE 1: Training with FROZEN image branch (20 epochs)
  Goal: Stabilize fusion layer before fine-tuning image
================================================================================
Epoch 11: early stopping
Restoring model weights from the end of the best epoch: 1.
  Stage 1 completed. Best val kappa: 0.0450
================================================================================
STAGE 2: Fine-tuning with UNFROZEN image branches (1 modalities)
  Image modalities to unfreeze: ['depth_rgb']
  Learning rate: 1e-6 (very low to prevent overfitting)
  Unfreezing image layers...
================================================================================
  Successfully unfrozen 14 layers across 1 modalities
  Model recompiled with LR=1e-6
Epoch 11: early stopping
Restoring model weights from the end of the best epoch: 1.
================================================================================
Two-stage training completed!
  Stage 1 (frozen):    Kappa 0.0450
  Stage 2 (fine-tune): Kappa 0.0450
  No improvement from fine-tuning (kept Stage 1 weights)
================================================================================

Run 2 Results for metadata+depth_rgb:
Cohen's Kappa: 0.1065

Confusion Matrix (validation):
        Predicted: I    P    R
Actual Inflam:  106   26   25
Actual Prolif:  200   49   53
Actual Remodl:   21    5   19

Training gating network for run 2...

Number of models: 1

Initializing gating network training...
Number of models: 1
Shape of first model predictions: (474, 3)
Shape of true labels: (474,)
Skipping gating network: only 1 model(s), need at least 2 to combine

Gating Network Results for Run 2:
Accuracy: 0.3452
F1 Macro: 0.3207
Kappa: 0.1065
Results saved to /workspace/DFUMultiClassification/results/csv/modality_results_averaged.csv
Saved metadata+depth_rgb predictions to /workspace/DFUMultiClassification/results/checkpoints/combo_pred_metadata_depth_rgb_run1_train.npy
Saved metadata+depth_rgb predictions to /workspace/DFUMultiClassification/results/checkpoints/combo_pred_metadata_depth_rgb_run1_valid.npy
Saved metadata+depth_rgb predictions to /workspace/DFUMultiClassification/results/checkpoints/combo_pred_metadata_depth_rgb_run2_train.npy
Saved metadata+depth_rgb predictions to /workspace/DFUMultiClassification/results/checkpoints/combo_pred_metadata_depth_rgb_run2_valid.npy
Results for metadata, depth_rgb appended to /workspace/DFUMultiClassification/results/csv/modality_combination_results.csv

All results saved to /workspace/DFUMultiClassification/results/csv/modality_combination_results.csv

================================================================================
FINAL SUMMARY - BEST MODALITY COMBINATIONS
================================================================================

Best by Accuracy:
  Modalities: metadata+depth_rgb
  Accuracy: 0.3838 ± 0.0386
  F1 Macro: 0.3612
  Kappa: 0.1535

Total combinations tested: 1
================================================================================

